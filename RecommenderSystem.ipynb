{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "48d72052-a4be-4a22-be9c-4c753bd0911e"
    }
   },
   "source": [
    "# Recommender System\n",
    "\n",
    "<h2>Content Based VS Collaborative Filetering</h2>\n",
    "\n",
    " Um dos métodos de desenvolvimento de um sistema de recomendação é o <b>content based filtering</b>. Ele se baseia em uma descrição do item, utilizando palavras chaves, assim como nas preferencias do usuário, para identificar o tipo de item que ele gosta. \n",
    " Outro métodos para desenvolver é atraves do chamado <b>collaborative filtering</b>, este método se baseia na obtenção e analise de uma grande quantidade de informações sobre comportamentos do usuário, e prever o que pode interessar a ele através de similaridades entre usuários. Uma grande vantagem do <b>collaborative filtering</b> é conseguir boas recomendações sem depender de um conteudo analizado e entendimento sobre o item, por parte do sistema. \n",
    " O <b>collaborative filtering</b> pode ser dividido em dois métodos, o <b>memory-based</b> e o <b>model-based</b>. O <b>model-based</b> utiliza a fatoração de matrizes, é um método de aprendizado não supervisionado para variaveis latententes e redução de dimensionalidade. A fatorização de matriz é muito usada em sistemas de recomendação por lidar melhor com escalabilidade e sparsity do que o <b>memory-based</b>. Nesta apresentação será implementado um <b>model-based collaborative filtering</b>, que utiliza o <b>SVD</b>.\n",
    " \n",
    "<h2>SVD</h2>\n",
    "\n",
    " SVD é um tecnica de fatoração de matrizes comumente utilizada para produzir aproximações low-rank. Dada uma matriz m x n A, com rank r, o SVD será definido como $$SVD(A) = U\\times S\\times V^t$$ onde U, S e V tem dimensões m × m, m × n e n × n, respectivamente.\n",
    " A matriz S é uma matriz diagonal, tendo apenas r entradas não nulas, o que faz da dimensão das 3 matrizes m x r, r x r e r x n, respectivamente. As entradas da diagonal(s1, s2, s3, ..., sr) tem uma propriedadec onde si > 0 e s1 ≥ s2 ≥ ... ≥ sr.\n",
    " As primeiras r colunas de U e V representam os autovetores ortogonais, associados a r autovalores não nulos de $$AA^t$$ e $$A^tA$$, respectivamente.\n",
    "\n",
    "\n",
    "![svd](img.jpg)\n",
    "\n",
    "\n",
    "<h2>Implementação</h2>\n",
    "\n",
    " O dataset utilizado na implementação e teste é o MovieLens 100k disponivel em https://grouplens.org/datasets/movielens/100k/ , ele contem 100k classificações de filmes de 943 usuários numa seleção de 1682 filmes.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=header)\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos os dados do data frame em treinamento e teste através da função cross_validation do SciKit-Learn, em seguida criamos matrizes com esses dados. Já que a sparsity dessas matrizes é muito grande, guardamos as matrizes no formato CSC, <b>Compressed Sparse Column</b>, para otimizar perfomance e uso de memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity 93.69533063577546%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "train_data, test_data = cross_validation.train_test_split(df, test_size=0.25)\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = float(line[3])\n",
    "train_data_matrix = csc_matrix(train_data_matrix, dtype=float)\n",
    "    \n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = float(line[3])\n",
    "test_data_matrix = csc_matrix(test_data_matrix, dtype=float)\n",
    "\n",
    "print('Sparsity ' + str((float(n_users*n_items)-100000.0)*100/ float(n_users*n_items)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos o SVD de maneira optimizada utilizando o sparsesvd (https://pypi.python.org/pypi/sparsesvd/) e aplicamos o algoritimo encontrado no paper \"Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems\" $$Pi,j =\\bar{ri} + U\\times \\sqrt{S}^t(i)\\times \\sqrt{S}(j)\\times V^t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple SVD Recommender MAE: 2.41735172239\n",
      "Papers SVD Recommender MAE: 2.14275511801\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sparsesvd import sparsesvd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "K = 10\n",
    "U, s, Vt = sparsesvd(csc_matrix(train_data_matrix, dtype=float), K)\n",
    "\n",
    "S = np.zeros((K, K), dtype=float)\n",
    "S_no_sqrt = np.zeros((K, K), dtype=float)\n",
    "for i in range(0, len(s)):\n",
    "    S_no_sqrt[i,i] = s[i]\n",
    "    S[i,i] = math.sqrt(s[i])\n",
    "    \n",
    "U = csc_matrix(np.transpose(U))\n",
    "S = csc_matrix(S)\n",
    "S_no_sqrt = csc_matrix(S_no_sqrt)\n",
    "Vt = csc_matrix(Vt)\n",
    "\n",
    "prediction = U * S_no_sqrt * Vt\n",
    "\n",
    "erro_pred = prediction.toarray()[test_data_matrix.toarray().nonzero()]\n",
    "erro_test = test_data_matrix.toarray()[test_data_matrix.toarray().nonzero()]\n",
    "\n",
    "print('Simple SVD Recommender MAE: ' + str(mean_absolute_error(erro_pred, erro_test)))\n",
    "\n",
    "\n",
    "# Paper's algorithm\n",
    "StSVt = np.transpose(S) * S * Vt\n",
    "\n",
    "estimated_ratings = np.zeros(shape=(n_users, n_items), dtype=float)\n",
    "for row in range(n_users):\n",
    "    pred = U[row, :] * StSVt\n",
    "    estimated_ratings[row, : ] = np.mean(train_data_matrix[row]) + pred.todense()\n",
    "\n",
    "erro_pred = estimated_ratings[test_data_matrix.toarray().nonzero()]\n",
    "erro_test = test_data_matrix.toarray()[test_data_matrix.toarray().nonzero()]\n",
    "\n",
    "print('Papers SVD Recommender MAE: ' + str(mean_absolute_error(erro_pred, erro_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
